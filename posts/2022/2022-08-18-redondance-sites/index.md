---
title: Redondance "du pauvre" d'un site personnel
categories:
- Infra
tags:
- ovh
- python
# https://www.iconfinder.com/icons/4263526/download/svg/4096
thumbnail: thumbnail.svg
---

# Contexte

Mon serveur h√©berg√© a des petits soucis en ce moment, il a tendance √† planter r√©guli√®rement malgr√© le fait que je le supervise activement.  
J'ai tr√®s longtemps pens√© que c'√©tait l'un de mes containers docker qui s'emballait (malgr√© le fait qu'il y ait des options de runtime de param√©tr√©es) mais il s'av√®re que c'est le metrics scraper d'OVH install√© qui posait probl√®me !  
Bref avant de r√©soudre le probl√®me, mes services h√©berg√©s ne r√©pondaient plus de temps en temps et cela reste tout de m√™me tr√®s emb√™tant. Il a donc fallu que je pense √† une potentielle redondance !

# Etude des solutions

Il fallait donc que je trouve une solution pour avoir les sites les plus importants toujours en fonction en cas de d√©faillance majeure de mon serveur d√©di√© le temps que je trouve une solution √† long terme. Pour simplifier la probl√©matique je ne souhaite basculer qu'un seul site pour le moment car il est consult√© fr√©quemment pour des raisons professionnelles, les autres n'ont pas d'importance cruciale.

J'ai les √©quipements suivants avec moi :
- Une IP publique d√©di√©e d'OVH
- Un serveur h√©berg√© chez OVH
- Un micro serveur avec des ressources √©quivalentes de celle d'OVH chez moi
- La fibre et une IP publique dynamique (celle de ma box)

Voici les quelques techniques auxquelles j'ai pens√© en premier lieu :

- Backup, Replica
- Cluster
- Solution de fail over

La solution **backup/r√©plica** a tout de suite √©t√© √©cart√©e du fait qu'elle ne soit pas du tout flexible. Je fais d√©j√† du backup de mon serveur vers chez moi mais c'est cela concerne plus la conf de mon dockerfile et les datas de certains containers mais rien qui ne me permet de remonter une infra en 10min automatiquement (surtout pour une solution cens√©e rester temporaire).  
Pour le **cluster** √ßa serait la solution id√©ale, dans mon cas passer sous Docker Swarm ou encore mieux tout migrer sous K8s/K3s. Le probl√®me est que je veux une solution rapide √† mettre en place ne necessitant pas une refonte compl√®te de mon infra. D'autant plus que je n'ai pas du tout r√©fl√©chi √† la conception de ma future infra et comment cela est g√©r√© avec une IP publique dynamique.  
La derni√®re proposition est surement la plus adapt√©e pour moi, trouver une **solution de failover** simple et efficace avec le moins d'intervention humaine sans toucher √† la configuration d√©j√† en place.

Une fois choisie il a fallu que je compare les solutions avec les contraintes suivantes :
- Une bascule faite en 5 minutes max en cas de probl√®me.
- L'ensemble des donn√©es doivent √™tre identique et synchronis√© (si modification d'un fichier html r√©plication imm√©diate sur le backup).
- Rapide et simple √† mettre en place, pas de service payant etc...

Apr√®s un moment de r√©flexion voici le workflow imagin√© :

![](./schema_failover.png)

Le d√©tail des flux serait donc le suivant :
- Probl√®me sur la production (serveur OVH) qui rend le site web indisponible
- Un script sur le serveur backup v√©rifie r√©guli√®rement la disponibilit√© du site web
- Le script constate l'indisponibilit√© et via les API OVH change l'IP publique du site en question en la faisant pointer sur le serveur de backup lui-m√™me
- Avec un TTL assez faible (60), l'utilisateur devrait avoir acc√®s au site web via le backup

Il reste le sujet de la synchronisation des donn√©es, solutions possibles :
- Un montage sur le dossier du site web en question
- Un rsync depuis le backup vers la prod √† une fr√©quence r√©guli√®re
- Un rsync depuis la prod vers le backup d√®s qu'un fichier est modifi√©

J'ai pass√© pas mal de temps sur ces solutions car elles ont toutes leurs probl√®mes :
- **Montage** : Aucun int√©r√™t car si la prod plante je n'ai plus de montage et donc plus de donn√©e, il faut forc√©ment une copie. Je peux toujours par la suite via un cron faire une copie r√©guli√®re des donn√©es en local mais aucun int√©r√™t par rapport aux autres solutions.
- **Rsync depuis le backup** : Pas trop fan de cr√©er des acc√®s sur mon serveur de prod m√™me si c'est du rsync via cl√© SSH sur un user non root. Mais surtout comment je d√©tecte qu'un fichier a chang√© ou √©t√© cr√©√© ? Seule la prod peut avoir ses infos. J'ai tout de m√™me essay√© de cr√©er un montage sshfs avec une surveillance du dossier via l'outil inotify mais cela ne marche pas du tout ensemble, inotify fonctionne avec un syst√®me de fichier local.
- **Rsync depuis la prod** : Derni√®re solution, celle-ci parait la plus "faisable" la prod surveillant ses fichiers via inotify ou un autre outil et rsync en cas de changement. Par contre avec l'IP dynamique je suis toujours "bloqu√©" car √† chaque red√©marrage de ma box je change d'IP. J'ai donc d√ª faire appel √† un service type DynDNS, ma box ne g√©rant que DynDNS ou NoIP j'ai choisi ce dernier qui a une version freemium (il faut valider manuellement le domaine tous les mois sinon 5‚Ç¨/mois...)

# Mise en place

J'ai 4 parties √† mettre en place :
- Le script de v√©rification de la disponibilit√© du site web.
- Le script de changement du DNS.
- L'outil qui va permettre la synchro des donn√©es de la prod vers le backup.
- Des petits scripts annexes pour alerter en cas de d√©faillance de la prod/backup.

Toute la partie scripting va √™tre condens√©e dans un seul et m√™me script

## Partie scripting

```
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import sys
import time
import requests
import dns.resolver
import ovh

new_ip = requests.get('https://api.ipify.org').content.decode('utf8') #Dynamic public IP
domain = "beerus.fr"
site_name = "<the_website_to_monitor>"
ifttt_event = "<event_ifttt_name>" #See https://ifttt.com
ifttt_key = "<event_ifttt_key>" #See https://ifttt.com
ovh_secret = "<ovh_secret>" #See https://docs.ovh.com/ca/fr/api/first-steps-with-ovh-api/
ovh_key = "<ovh_key>" #See https://docs.ovh.com/ca/fr/api/first-steps-with-ovh-api/
ovh_consumer = "<ovh_consumer>" #See https://docs.ovh.com/ca/fr/api/first-steps-with-ovh-api/

# setting the URL you want to monitor
URL = Request("https://{0}.{1}".format(site_name, domain),
              headers={'User-Agent': 'Mozilla/5.0'})
i = 0

#Verify status of the DNS
result = dns.resolver.query(site_name, 'A')
for ipval in result:
    if ipval.to_text() == new_ip:
        print("Already switched")
        sys.exit(0)

#The site is answering ?
while i < 10:
    time.sleep(5)
    print(URL.full_url)
    try:
        response = urlopen(URL, timeout=2)
    except (HTTPError, URLError) as error:
        i = i+1
        continue
    if response.status == 200:
        print("Prod is okay")
        break
    i = i+1

#The website is down so we can switch and alert
if i == 10:
    response = requests.post('https://maker.ifttt.com/trigger/{0}/with/key/{1}'.format(ifttt_event, ifttt_key))
    client = ovh.Client(
        endpoint='ovh-eu',
        application_key=ovh_key,
        application_secret=ovh_secret,
        consumer_key=ovh_consumer,
    )

    result = client.put(
        '/domain/zone/{0}/record/5206233547'.format(domain),
        subDomain=site_name,
        target=new_ip,
        ttl=60,
    )

    result = client.post('/domain/zone/{0}/refresh'.format(domain))
```

Le script ci-dessus a √©t√© pass√© en cron sur le serveur backup et est ex√©cut√© toutes les 10 minutes.  
Il ex√©cute les actions suivantes :
- V√©rification du DNS (le site a-t-il d√©j√† bascul√© ?)
- Curl vers le site en question, 10 tests avec un intervalle de 5s
- Si down bascule vers le backup via les API d'OVH
- Notification (on verra √ßa dans une autre partie)

## Synchronisation donn√©es

Pour la synchro j'avais dans un premier temps pens√© √† inotify avec rsync.  
Il aurait suffi de lancer le binaire (via systemd dans l'id√©al) et d√®s lors qu'il aurait remarqu√© un event qui match avec les options pr√©cis√©es (create, delete, move etc...) il effectuerait un rsync.  
Le probl√®me √©tant que cela n√©cessite pas mal de bricolage (script, cr√©ation d'un systemd etc...) et j'aurais aim√© en √©viter le plus possible.
Heureusement je suis tomb√© sur l'outil **lsyncd** qui fait exactement ce travail, pas besoin de r√©inventer la roue !
En plus il peut se baser sur rsync pour faire ses actions ce qui est parfait pour moi. J'ai fait deux configurations :
- Une pour le webroot de mon site, en cas de modification/ajout/suppression lsyncd ex√©cute un rsync imm√©diatement vers mon backup
- Une pour le dossier des certificats de mon site web, √©tant donn√© que je suis sous LetsEncrypt les certificats sont renouvel√©s r√©guli√®rement. De la m√™me fa√ßon d√®s qu'il y a une modification de ceux-ci (le certificat public √† priori) il copie le nouveau certificat sur le backup

Tous cela est parfait mais il me manque encore la partie SSH pure car mon serveur backup est h√©berg√© chez moi derri√®re ma box. J'utilise d√©j√† le port 22 pour d'autres usages donc il va falloir faire de la configuration additionnelle.  
J'avais deux possibilit√©s :
- Je fais un NAT sur un port diff√©rent du 22 (du style IP_PUB:2222 --> backup:22)
- Je fais un bastion SSH avec un hardenning et qui forward mes requ√™tes vers mon backup

J'ai d√©cid√© de faire la deuxi√®me solution, √ßa permet de s'entrainer sur quelque chose que je n'ai pas l'habitude de faire üôÇ. Je passe la partie hardenning car ce n'est pas ce que j'ai envie d'aborder dans ce post mais √ßa consiste √† faire les choses suivantes (non exhaustives) :
- Fail2ban
- No PermitRoot
- Authentification par cl√© uniquement
- Un Jail chroot avec un nombre de binaire limit√© (adios su et sudo üòà)

Par rapport au bastion, 3 solutions pour faire ce jump :
- Stockage des cl√©s SSH sur le bastion
- Forwarding SSH agent
- Utiliser ProxyCommand/ProxyJump


**Stockage des cl√©s SSH sur le bastion**: Cela signifie que le bastion poss√®de la cl√© SSH priv√©e de mon serveur backup, √©tant donn√© qu'il reste public cela est toujours dangereux. On met cette id√©e de c√¥t√© pour l'instant.  
**Forwarding SSH agent**: On passe nos cl√©s priv√©es √† un agent SSH puis on forward cet agent sur notre bastion. L'agent SSH √©tant une socket Unix (li√© √† la variable $SSH_AUTH_SOCK) le jump host va pointer vers notre agent local au lieu de pointer sur le sien. Mais il suffit d'√™tre root sur le bastion pour faire pointer sa variable $SSH_AUTH_SOCK sur notre socket et acc√©der au backup. Pas terrible niveau s√©cu √©galement.  
**ProxyCommand/ProxyJump**: La meilleure solution, le serveur interm√©diaire (le bastion) ex√©cute une `ProxyCommand` lors de la connexion et forward l'input et l'output vers le backup. Il suffit pour cela d'utiliser l'option `-J` du binaire SSH.  
Ex : `ssh -J username@bastion username@backup`. Le seul probl√®me de cette solution c'est que si je veux pr√©ciser une cl√© SSH priv√©e la commande ne peut pas deviner si cela s'applique au bastion ou au backup. Je suis donc pass√© via la config SSH :

```
Host bastion
    User linux
    HostName bastion.domain.tld
    IdentityFile ~/.ssh/bastion_rsa

Host backup #Pas besoin d'√™tre accessible depuis internet donc
    User backup
    HostName backup.domain.lan
    IdentityFile ~/.ssh/backup_rsa
    ProxyJump bastion.domain.tld
```

Et lorsque je fais un simple `ssh backup` (apr√®s recopie des cl√©s publiques sur les bons serveurs) la magie op√®re ! Petit test de modification d'un fichier √ßa fonctionne bien, le fichier modifi√© est bien propag√© sur le backup üôÇ

## Notifications

Dernier point : le fignolage ! Il s'agit d√©sormais de m'avertir en cas de d√©faillance de la prod OU du backup (on ne sait jamais lui aussi il peut planter !).  
Pour le backup je fais presque que la m√™me chose que le script ci-dessus, j'ai des curl qui interroge r√©guli√®rement le nom DynDNS pointant vers mon site web (j'ai configur√© mon reverse proxy Nginx pour r√©pondre au vrai nom de mon site web et au nom DynDNS). En cas de d√©faillance je re√ßois un mail sur ma boite perso @beerus.fr.  

Par contre pour la prod c'est plus complexe (et plus important aussi). En fait je pourrais faire le m√™me m√©canisme que pour le backup mais je ne recevrai jamais le mail @beerus.fr car mon serveur mail est h√©berg√© sur la m√™me machine que mon site web...  
J'ai commenc√© √† regarder pour pouvoir utiliser les API de Gmail mais tr√®s honnetement √ßa ne me tentait pas trop.  
Donc √† la place j'ai utilis√© le service **IFTTT**, j'ai trouv√© le concept tellement simple et efficace.
Une application Android (ou Apple) √† t√©l√©charger ensuite "programmer" son alerte :

![](./ifttt.png)

Comme on peut le voir ci-dessus en cas de r√©ception d'un webhook on envoie une notification push √† l'application IFTTT reli√©e au compte.  
J'ai juste eu √† ajouter la commande curl √† mon script ci-dessus et le tour √©tait jou√©, √ßa marche parfaitement ! Tr√®s peu d'infos sont √©galement transmise √† ce service tiers (surement une ip publique et peut-√™tre des infos n√©gligables concernant mon t√©l√©phone) √ßa reste acceptable de mon c√¥t√©, le businness plan consiste √† proposer des version premium qui permettent de d√©velopper son propre algo ou avoir plus d'event.

# Conclusion

Une grosse journ√©e de travail pour concevoir, mettre en place et peaufiner. J'esp√©rais pas mettre plus de temps et c'est chose faite.
J'ai une solution, certes bricol√©e, mais fonctionnelle et facile. Il me reste tout de m√™me une partie manuelle : en cas de d√©faillance de la prod et de bascule sur le backup, je dois manuellement refaire pointer le DNS vers la prod (bien que j'utilise un script pour √ßa).
La solution n'est de toute fa√ßon mise en place que temporairement le temps que je r√©fl√©chisse √† une nouvelle architecture qui sera bien plus robuste et pens√©e en amont.  
Cela reste malgr√© tout un tr√®s bon exercice, tr√®s proche du syst√®me √ßa toujours fait du bien de remettre les mains dans le cambouis üôÇ