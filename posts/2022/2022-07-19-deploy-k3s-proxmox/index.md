---
title: D√©ployer son infra K3S sur Proxmox IaC
categories:
- Infra
tags:
- k3s
- hashicorp
- packer
- terraform
- proxmox
# https://cncf-branding.netlify.app/img/projects/k3s/icon/color/k3s-icon-color.svg
thumbnail: thumbnail.svg
---

# Architecture

je suis en train de pr√©parer la Certified Kubernetes Administrator (CKA) et il me faudrait un lab pour m'exercer. J'ai pas mal fait de Kubernetes classique en lab via des foramtions Udemy et j'aimerai tester autre chose mais toujours sur une base K8S. 
Je me suis donc pench√© sur K3S de Rancher. Il dispose de plusieurs qualit√© qui me semble int√©ressantes : 
- Il est l√©ger et plus rapide que K8s
- Il peut s'ex√©cuter sur du plus petit mat√©riel (proc ARM par ex ü•∞). J'ai comme projet potentiel de faire un cluster de raspberry donc K3S est plus qu'indiqu√©.
- Il ne dispose pas de tous les connecteurs cloud mais vu que √ßa sera du on premise dans mon cas c'est parfait.
- Plein de petits avantages : + facile et rapide √† d√©ployer, moins de surface d'attaque, facile √† update etc...

Par contre il ne fait pas tourner docker nativement mais containerd (bien que j'ai vu un projet passer s'appelant k3d qui int√®gre docker), √ßa sera l'occasion d'apprendre autre chose surtout que le CRI (Container Runtime Interface) en soi n'est pas le plus important.

Une fois mon choix d'orchestrateur choisi je me suis dit que j'allais installer mon cluster K3s sur mon lab maison qui ex√©cute un Proxmox et tant qu'√† faire autant avoir un workflow de d√©ploiement un peu √©volu√© pour s'√©xercer.

Parmi toutes les ressources que j'ai pu voir celle qui me semblait au d√©part le plus int√©ressant faisait les choses suivantes : 
- R√©cup√©ration de l'ISO Ubuntu 20 sur Proxmox
- Cr√©ation du template Proxmox "√† la main" via des commandes qm (cli proxmox)
- Configuration de cloud init via l'onglet promox d√©di√©
- Cr√©ation des VM via Terraform 
- Ansible pour d√©poyer K3s sur mes nouvelles VM

Ca paraissait sympa sur le papier et puis √ßa permettait de mettre en application les quelques connaissances Terraform dont je disposais. 
Mais √† bien r√©fl√©chir il y avait quand m√™me deux probl√®mes dans ce process : 
- Une partie manuelle qui cassait un peu l'automatisation voulue. Si je pouvais automatiser cela je serai vraiment dans un cas d'IaC (**Infra As Code**)
- Pas tr√®s flexible cat si je veux customiser mon template √ßa sera √† la main aussi ou a la limite via script bash

C'est l√† que je suis tomb√© sur un autre outil d'Hashicorp qui fait tout ce travail manuel pour moi et dispose de la flexibilit√© voulue : Packer !

</br>Voici les deux workflow qui sont ressortis au final : 

- Creation du template --> Cloud init --> Terraform --> Ansible --> K3S  
- **Packer (+Cloud init) --> Terraform --> Ansible --> K3S**

J'ai donc choisi le deuxi√®me worflow  
Avantages :
- Infra as code - Full automatis√©e
- Pas d'int√©raction directe avec Proxmox, seulement via son API
- Peut donc √™tre pilot√© depuis une machine tiers

Inconv√©nients:
- ‚ö†Ô∏èNecessite plus de developpement et de temps pour un r√©sultat √©quivalement dans mon cas (installation d'un potentiel DHCP, creation du fichier de conf packer etc...)
- Temps d'ex√©cution de cr√©ation du template plus long

Retrouver l'ensemble du projet sur ce git : https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer

# Packer

Je vais donc utiliser packer pour packager mon image ubuntu 20.04 avec quelques packages et conf additionnelles. Il suffit d'installer [packer](https://www.packer.io/downloads) puis nous verrons les fichiers de configuration.  
D'ailleurs dans mon cas pas besoin de DHCP, celui de ma box avec ma VM template en mode bridge sera largement suffisant.  
Voici mon aborescence packer pour la config : 
```
‚îú‚îÄ‚îÄ http
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ meta-data
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ user-data
‚îú‚îÄ‚îÄ ubuntu20.pkr.hcl
‚îî‚îÄ‚îÄ variables.pkr.hcl
```

Les fichiers de conf peuvent √™tre en json ou HCL (format hashicrop), j'ai choisi HCL √† la place de JSON pour deux raisons : 
- Language commun avec tous les outils HashiCorp (vu qu'on va aussi utiliser Terraform √ßa a du sens)
- Plus "fonctionnel", on peut par exemple mettre des commentaires (dans JSON non √ßa fera partie de la data)

On a donc 3 fichiers hcl concernant : 
- `variables.pkr.hcl` : la d√©finition des variables par d√©faut
- `ubuntu20.pkr.hcl` : la d√©finition du job packer

Regardons plus en d√©tail `ubuntu20.pkr.hcl` : 

```ruby
source "proxmox" "template" {
  proxmox_url = "${var.proxmox_hostname}/api2/json"
  username = var.proxmox_username
  password = var.proxmox_password
  node = var.proxmox_node_name
  insecure_skip_tls_verify = var.proxmox_insecure_skip_tls_verify
  network_adapters {
    bridge = "vmbr0"
  }
  disks {
    type = "scsi"
    disk_size = "20G"
    storage_pool = var.vm_storage_pool
    storage_pool_type = "lvm"
  }
  #iso_file = "local:iso/ubuntu-20.04.4-live-server-amd64.iso"
  iso_url               = var.iso_url
  iso_storage_pool      = var.iso_storage_pool
  iso_checksum          = var.iso_checksum
  unmount_iso = true
  boot_wait = "5s"
  memory = var.vm_memory
  sockets   = var.vm_sockets
  cores     = var.vm_cores
  template_name = var.vm_name
  vm_id     = var.vm_id
  http_directory = var.http_directory
  cloud_init = true
  cloud_init_storage_pool = var.iso_storage_pool
  boot_command = [
    "<esc><wait><esc><wait><f6><wait><esc><wait>",
    "<bs><bs><bs><bs><bs>",
    "autoinstall ds=nocloud-net;s=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ ",
    "--- <enter>"
  ]
  ssh_username = var.username
  ssh_password = var.user_password
  ssh_timeout = "20m"
}

build {
  sources = [
    "source.proxmox.template"
  ]
  provisioner "shell" {
    inline = [
      "sudo rm -f /etc/cloud/cloud.cfg.d/99-installer.cfg",
      "sudo rm -f /etc/cloud/cloud.cfg.d/subiquity-disable-cloudinit-networking.cfg",
      "sudo cloud-init clean"
    ]
  }
}
```
Rien de bien compliqu√© l√† dedans c'est aussi l'avantage en g√©n√©ral des outils Hashicorp la configuration est tr√®s descriptive et donc compr√©hensible rapidement.
On va renseigner les informations suivantes : 
- Des credentials pour le proxmox
- Un peu de hardware pour le template (RAM, CPU, Disk etc...)
- Une config pour le template (ID, nom, boot command , ssh cred etc...)
- Un dossier pour la conf subiquity
- **On active cloud-init car sinon on ne peut pas set les IP via Terraform**
- On tweak un peu l'image car on veut qu'elle soit cloud-init ready

Justement concernant autoinstall depuis la version 20.04 preseed a √©t√© d√©laiss√© au profit de subiquity qui est (de mon point de vue) bien plus facile √† utiliser car format yaml et s'int√®gre tr√®s bien avec Packer. Ce qui donne deux fichiers : 
- `meta-data` : requis. Utilis√© par le cloud vu qu'on d√©ploit en local on le laisse vide
- `user-data` : l'√©quivalent du preseed, utilise autoinstall

Le fichier `user-data` en d√©tail :
```yml
#cloud-config
autoinstall:
  version: 1
  locale: en_US
  keyboard:
    layout: fr
  ssh:
    install-server: true
    allow-pw: true
  packages:
    - qemu-guest-agent
  user-data:
    users:
        - name: canadium
          passwd: $6$exDY1mhS4KUYCE/2$zmn9ToZwTKLhCw.b4/b.ZRTIZM30JZ4QrOQ2aOXJ8yk96xpcCof0kxKwuX1kqLG/ygbJ1f8wxED22bTL4F46P0
          groups: [adm, cdrom, dip, plugdev, sudo]
          lock-passwd: false
          sudo: ALL=(ALL) NOPASSWD:ALL
          shell: /bin/bash
          ssh_authorized_keys:
            - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJEXrwiuUOCpWPvwOsGuF4K+aq1ufToGMi4ra/1omOZb
```

Fichier de configuration ultra basique, on d√©finit un seul user et deux conf pour le syst√®me. Les autres options peuvent √™tre trouv√©es dans la [doc](https://ubuntu.com/server/docs/install/autoinstall-reference)  
‚ö†Ô∏è Ce fichier est "templatiser" et red√©fini via Terraform voir plus bas

On peut suite √©x√©cutera ensuite packer via Terraform pour avoir une seule et m√™me ex√©cution  
Ce qui nous donne un template de qualit√© ! 
![](./template-proxmox.png)

# Terraform

Il faut maintenant d√©ployer notre templates sous forme de VM, on va utiliser terraform pour cela qui va √©galement nous pr√©parer nos fichiers ansible.
On installe [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started) puis on s'attaque aux fichier de configuration : 
```
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ output.tf
‚îú‚îÄ‚îÄ provider.tf
‚îú‚îÄ‚îÄ terraform.tfvars
‚îî‚îÄ‚îÄ variables.tf
```

Listons les fichiers terraform :
- `main.tf` : On d√©crit notre d√©ploiement, le fichier de job.
- `output.tf` : La sortie voulue lors de l'ex√©cution de terraform apply (on va print les IP ici)
- `provider.tf` : On pr√©cise quel provider on utilise, ici proxmox. (voir [doc](https://registry.terraform.io/providers/Telmate/proxmox/latest/docs))
- `terraform.tfvars` : la d√©finition des variables pour le main.tf (m√™me principe que Packer)
- `variables.tf` : la d√©finition des variables par d√©faut (m√™me principe que Packer)

Et on a des fichiers de template qui va nous permettre de d√©finir notre inventory ansible, groups_vars ansible, user-data de autoinstall etc...

Je vais juste d√©crire le fichier main.tf car les autres sont plut√¥t √©vident. 
On d√©finit les variables pour l'ex√©cution future de packer : 

```ruby
locals {
  template_folder = "${path.module}/${var.folder_packer}/${var.distrib_folder}"
  packer_cfg = {
    PKR_VAR_proxmox_hostname  = var.pm_host
    PKR_VAR_proxmox_username  = var.pm_user
    PKR_VAR_proxmox_password  = var.pm_password
    PKR_VAR_proxmox_node_name = var.pm_node_name
    PKR_VAR_proxmox_insecure_skip_tls_verify = var.pm_tls_insecure

    PKR_VAR_vm_id                 = var.vm_id
    PKR_VAR_vm_name               = var.vm_name
    PKR_VAR_vm_storage_pool       = var.vm_storage_pool
    PKR_VAR_vm_cores              = var.vm_cores
    PKR_VAR_vm_memory             = var.vm_memory
    PKR_VAR_vm_sockets            = var.vm_sockets

    PKR_VAR_iso_url             = var.iso_url
    PKR_VAR_iso_storage_pool    = var.iso_storage_pool
    PKR_VAR_iso_checksum        = var.iso_checksum

    PKR_VAR_http_directory      = var.http_directory

    PKR_VAR_username              = var.username
    PKR_VAR_user_password         = var.user_password
  }
}
```
Ici justement on d√©finit le job packer avec l'ensemble des param√®tres souhait√©s (au final c'est une ex√©cution shell classique).  
J'ai ajout√© un petit sleep car parfois le template n'√©tait pas pr√™t pour l'ex√©cution Terraform. Egalement un script python pour supprim√© le template cr√©√© par Packer (pas de ressource native donc oblig√© de "hack") : 
```ruby
resource "null_resource" "packer_build" {
    provisioner "local-exec" {
    working_dir = local.template_folder
    command     = "packer build . && sleep 30"
    environment = local.packer_cfg
  }
  provisioner "local-exec" {
    when  = destroy
    command = "${path.module}/scripts/delete_template.py"
    interpreter = ["python"]
    working_dir = path.module
    environment = merge(yamldecode(self.triggers.packer_cfg))
  }
    triggers = {
    packer_cfg = yamlencode(local.packer_cfg)
  }
  depends_on = [
    local_file.user-data
  ]
}
```

On red√©finit un master K3S par dessus notre template (var.tamplate_vm_name), on remarque qu'on doit pr√©ciser une d√©pendance pour que les t√¢ches d'ex√©cutent dans le bon ordre. A noter aussi un script sh qui verifiera que le d√©ploiement cloud-init est bien fini sur le serveur :
```ruby
resource "proxmox_vm_qemu" "proxmox_vm_master" {
  count       = var.num_k3s_masters
  name        = "k3s-master-${count.index}"
  target_node = var.pm_node_name
  clone       = var.template_vm_name
  os_type     = "cloud-init"
  agent       = 1
  memory      = var.num_k3s_masters_mem
  cores       = var.num_k3s_masters_cpu

  ipconfig0 = "ip=${var.master_ips[count.index]}/${var.networkrange},gw=${var.gateway}"

  lifecycle {
    ignore_changes = [
      ciuser,
      sshkeys,
      disk,
      desc,
      network
    ]
  }

  connection {
    type = "ssh"
    user = var.username
    password = var.user_password
    host = var.worker_ips[count.index]
  }

  provisioner "file" {
    source      = "${path.module}/scripts/wait-cloud-init.sh"
    destination = "/tmp/wait-cloud-init.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/wait-cloud-init.sh",
      "/tmp/wait-cloud-init.sh",
    ]
  }
  depends_on = [
    null_resource.packer_build
  ]
}
```

On fera la m√™me t√¢che pour les workers

Et ici on d√©finit nos t√¢ches de customisation de conf, c'est √† dire :
- G√©n√©rer des fichiers de configuration pour la future ex√©cution d'ansible
- G√©n√©rer le user-data necessaire √† l'autoinstall de packer (on modifie essentiellement le user/password)

```ruby
data "template_file" "cloud-init-user-data" {
    template = "${file("${path.module}/templates/user-data.tpl")}"
    vars = {
        SUDO_PASSWORD_HASH = "${var.user_password_hash}"
        SUDO_USERNAME = "${var.username}"
        SSH_PUBLIC_KEY = "${var.ssh_pub_key}"
    }
}
resource "local_file" "user-data" {
    content     = "${data.template_file.cloud-init-user-data.rendered}"
    filename = "${local.template_folder}/http/user-data"
}

data "template_file" "k8s" {
  template = file("./templates/k8s.tpl")
  vars = {
    k3s_master_ip = "${join("\n", [for instance in proxmox_vm_qemu.proxmox_vm_master : join("", [instance.default_ipv4_address, " ansible_ssh_private_key_file=", var.pvt_key])])}"
    k3s_node_ip   = "${join("\n", [for instance in proxmox_vm_qemu.proxmox_vm_workers : join("", [instance.default_ipv4_address, " ansible_ssh_private_key_file=", var.pvt_key])])}"
  }
}

resource "local_file" "k8s_file" {
  content  = data.template_file.k8s.rendered
  filename = "${path.module}/ansible/hosts"
}

data "template_file" "groups_vars_ansible" {
    template = "${file("${path.module}/templates/all.tpl")}"
    vars = {
        ANSIBLE_USER = "${var.username}"
        K3S_VERSION = "${var.k3s_version}"
    }
}

resource "local_file" "groups_vars_ansible" {
    content     = "${data.template_file.groups_vars_ansible.rendered}"
    filename = "${path.module}/ansible/group_vars/all.yml"
}
```
Et enfin le job ansible : 

```ruby
resource "null_resource" "ansible-playbook" {
  provisioner "local-exec" {
    command = "ansible-playbook -i ${var.inventory_file}  --private-key ${var.ssh_key_file} site.yml"
    working_dir = ".."
  }
  depends_on = [
    proxmox_vm_qemu.proxmox_vm_workers,
    proxmox_vm_qemu.proxmox_vm_master,
    local_file.k8s_file,
    local_file.var_file
  ]
}
```

# Ansible

On va enfn d√©ployer le cluster K3S via ansible. En voici l'aborescence : 
```
ansible/
‚îú‚îÄ‚îÄ ansible.cfg
‚îú‚îÄ‚îÄ group_vars
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ all.yml
‚îú‚îÄ‚îÄ playbook.yml
‚îî‚îÄ‚îÄ roles
    ‚îú‚îÄ‚îÄ download
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tasks
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ main.yml
    ‚îú‚îÄ‚îÄ k3s
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ master
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ tasks
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ templates
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ k3s.service.j2
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ k3s.service.j2.withoutterafic
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ node
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ tasks
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ templates
    ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ k3s.service.j2
    ‚îú‚îÄ‚îÄ postconfig
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ localhost
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ tasks
    ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ main.yml
    ‚îú‚îÄ‚îÄ prereq
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ defaults
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ tasks
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ templates
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ resolv.conf.j2
    ‚îú‚îÄ‚îÄ raspberrypi
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ handlers
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ tasks
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ main.yml
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ prereq
    ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ CentOS.yml
    ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ Raspbian.yml
    ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ Ubuntu.yml
    ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ default.yml
```

Voici les diff√©rents roles : 
- `download` : T√©l√©charge la release de k3s
- `k3s` : La configuration de k3s pour le master et les nodes (rien de bien compliqu√©)
- `postconfig` : On configure kubeconfig et installe helm en local sur le server/workstation executant le job ansible
- `prereq` : On installe les prerequis K3S (netfilter, ip forwarding etc...)
- `raspberrypi` : Un job specifique si on est sous Raspberry PI

A noter : 
- Le fichier de group vars all.yml est g√©n√©r√© via Terraform √† partir de variables
- Le fichier ansible.cfg peut √™tre configurer √† votre convenance


# Conclusion 
On a donc cr√©√© un cluster K3s de 0 sans aucune interaction directe ou manuelle avec Proxmox, pas mal non ? En plus de cela tous est dynamique et flexible, il suffit de changer les conf üòâ

![](./final_promox.png)


Vous pouvez reprendre ce [projet](https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer) (qui lui m√™me est fork√©). Par rapport √† son utilisation tout est expliqu√© dans le [README](https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer/blob/main/terraform/README.md)
 et consiste principalement √† changer des variables Terraform.

Malgr√© tous cela il y a pas mal d'axes d'am√©liorations et probl√®mes inh√©rents √† la solution choisie : 
- On pourrait mettre les variables sensibles dans Vault (Outil Hashicorp) et un git qui √† chaque push red√©ploit une image packer.
- Il faudrait aussi cr√©√©r les utilisateurs adapt√©s (terraform & packer) avec les bons droits, voir ressource [Terraform](https://registry.terraform.io/providers/Telmate/proxmox/latest/docs) par exemple.  
- Il faudrait que le job packer se termine au bon moment pour que le job terraform ne se lance pas trop t√¥t (j'ai un sleep 30 pour l'instant)  
- Pas de provider packer natif donc on doit "tricher" pour supprimer le template
- Terraform consid√©re √† un deuxi√®me run que rien a chang√© et ne relance pas le job ansible (on peut tout de m√™me le lancer √† la main)

Bref il y a plein d'autres moyens de faire et d'am√©liorer ce pipeline, les outils HashiCorp sont quand m√™me super pour cela !

# Sources

https://www.aerialls.eu/posts/ubuntu-server-2004-image-packer-subiquity-for-proxmox/  
https://tlhakhan.medium.com/ubuntu-server-20-04-autoinstall-2e5f772b655a  
https://stackoverflow.com/questions/72567455/running-cloud-init-twice-via-packer-terraform-on-vmware-ubuntu-22-04-guest  
https://salmonsec.com/blogs/home_lab_3#6-init-kubeadm-images-sh  
https://github.com/blz-ea/proxmox-packer  
https://medium.com/@ssnetanel/build-a-kubernetes-cluster-using-k3s-on-proxmox-via-ansible-and-terraform-c97c7974d4a5