---
title: DÃ©ployer son infra K3S sur Proxmox IaC
categories:
- Infra
tags:
- k3s
- hashicorp
- packer
- terraform
- proxmox
# https://www.reshot.com/free-svg-icons/item/sharing-C6F57JEL49/
thumbnail: thumbnail.svg
---

# Architecture

je suis en train de prÃ©parer la Certified Kubernetes Administrator (CKA) et il me faudrait un lab pour m'exercer. J'ai pas mal fait de Kubernetes classique en lab via des foramtions Udemy et j'aimerai tester autre chose mais toujours sur une base K8S. 
Je me suis donc penchÃ© sur K3S de Rancher. Il dispose de plusieurs qualitÃ© qui me semble intÃ©ressantes : 
- Il est lÃ©ger et plus rapide que K8s
- Il peut s'exÃ©cuter sur du plus petit matÃ©riel (proc ARM par ex ğŸ¥°). J'ai comme projet potentiel de faire un cluster de raspberry donc K3S est plus qu'indiquÃ©.
- Il ne dispose pas de tous les connecteurs cloud mais vu que Ã§a sera du on premise dans mon cas c'est parfait.
- Plein de petits avantages : + facile et rapide Ã  dÃ©ployer, moins de surface d'attaque, facile Ã  update etc...

Par contre il ne fait pas tourner docker nativement mais containerd (bien que j'ai vu un projet passer s'appelant k3d qui intÃ¨gre docker), Ã§a sera l'occasion d'apprendre autre chose surtout que le CRI (Container Runtime Interface) en soi n'est pas le plus important.

Une fois mon choix d'orchestrateur choisi je me suis dit que j'allais installer mon cluster K3s sur mon lab maison qui exÃ©cute un Proxmox et tant qu'Ã  faire autant avoir un workflow de dÃ©ploiement un peu Ã©voluÃ© pour s'Ã©xercer.

Parmi toutes les ressources que j'ai pu voir celle qui me semblait au dÃ©part le plus intÃ©ressant faisait les choses suivantes : 
- RÃ©cupÃ©ration de l'ISO Ubuntu 20 sur Proxmox
- CrÃ©ation du template Proxmox "Ã  la main" via des commandes qm (cli proxmox)
- Configuration de cloud init via l'onglet promox dÃ©diÃ©
- CrÃ©ation des VM via Terraform 
- Ansible pour dÃ©poyer K3s sur mes nouvelles VM

Ca paraissait sympa sur le papier et puis Ã§a permettait de mettre en application les quelques connaissances Terraform dont je disposais. 
Mais Ã  bien rÃ©flÃ©chir il y avait quand mÃªme deux problÃ¨mes dans ce process : 
- Une partie manuelle qui cassait un peu l'automatisation voulue. Si je pouvais automatiser cela je serai vraiment dans un cas d'IaC (**Infra As Code**)
- Pas trÃ¨s flexible cat si je veux customiser mon template Ã§a sera Ã  la main aussi ou a la limite via script bash

C'est lÃ  que je suis tombÃ© sur un autre outil d'Hashicorp qui fait tout ce travail manuel pour moi et dispose de la flexibilitÃ© voulue : Packer !

</br>Voici les deux workflow qui sont ressortis au final : 

- Creation du template --> Cloud init --> Terraform --> Ansible --> K3S  
- **Packer (+Cloud init) --> Terraform --> Ansible --> K3S**

J'ai donc choisi le deuxiÃ¨me worflow  
Avantages :
- Infra as code - Full automatisÃ©e
- Pas d'intÃ©raction directe avec Proxmox, seulement via son API
- Peut donc Ãªtre pilotÃ© depuis une machine tiers

InconvÃ©nients:
- âš ï¸Necessite plus de developpement et de temps pour un rÃ©sultat Ã©quivalement dans mon cas (installation d'un potentiel DHCP, creation du fichier de conf packer etc...)
- Temps d'exÃ©cution de crÃ©ation du template plus long

Retrouver l'ensemble du projet sur ce git : https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer

# Packer

Je vais donc utiliser packer pour packager mon image ubuntu 20.04 avec quelques packages et conf additionnelles. Il suffit d'installer [packer](https://www.packer.io/downloads) puis nous verrons les fichiers de configuration.  
D'ailleurs dans mon cas pas besoin de DHCP, celui de ma box avec ma VM template en mode bridge sera largement suffisant.  
Voici mon aborescence packer pour la config : 
```
â”œâ”€â”€ http
â”‚Â Â  â”œâ”€â”€ meta-data
â”‚Â Â  â””â”€â”€ user-data
â”œâ”€â”€ ubuntu20.pkr.hcl
â””â”€â”€ variables.pkr.hcl
```

Les fichiers de conf peuvent Ãªtre en json ou HCL (format hashicrop), j'ai choisi HCL Ã  la place de JSON pour deux raisons : 
- Language commun avec tous les outils HashiCorp (vu qu'on va aussi utiliser Terraform Ã§a a du sens)
- Plus "fonctionnel", on peut par exemple mettre des commentaires (dans JSON non Ã§a fera partie de la data)

On a donc 3 fichiers hcl concernant : 
- `variables.pkr.hcl` : la dÃ©finition des variables par dÃ©faut
- `ubuntu20.pkr.hcl` : la dÃ©finition du job packer

Regardons plus en dÃ©tail `ubuntu20.pkr.hcl` : 

```ruby
source "proxmox" "template" {
  proxmox_url = "${var.proxmox_hostname}/api2/json"
  username = var.proxmox_username
  password = var.proxmox_password
  node = var.proxmox_node_name
  insecure_skip_tls_verify = var.proxmox_insecure_skip_tls_verify
  network_adapters {
    bridge = "vmbr0"
  }
  disks {
    type = "scsi"
    disk_size = "20G"
    storage_pool = var.vm_storage_pool
    storage_pool_type = "lvm"
  }
  #iso_file = "local:iso/ubuntu-20.04.4-live-server-amd64.iso"
  iso_url               = var.iso_url
  iso_storage_pool      = var.iso_storage_pool
  iso_checksum          = var.iso_checksum
  unmount_iso = true
  boot_wait = "5s"
  memory = var.vm_memory
  sockets   = var.vm_sockets
  cores     = var.vm_cores
  template_name = var.vm_name
  vm_id     = var.vm_id
  http_directory = var.http_directory
  cloud_init = true
  cloud_init_storage_pool = var.iso_storage_pool
  boot_command = [
    "<esc><wait><esc><wait><f6><wait><esc><wait>",
    "<bs><bs><bs><bs><bs>",
    "autoinstall ds=nocloud-net;s=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ ",
    "--- <enter>"
  ]
  ssh_username = var.username
  ssh_password = var.user_password
  ssh_timeout = "20m"
}

build {
  sources = [
    "source.proxmox.template"
  ]
  provisioner "shell" {
    inline = [
      "sudo rm -f /etc/cloud/cloud.cfg.d/99-installer.cfg",
      "sudo rm -f /etc/cloud/cloud.cfg.d/subiquity-disable-cloudinit-networking.cfg",
      "sudo cloud-init clean"
    ]
  }
}
```
Rien de bien compliquÃ© lÃ  dedans c'est aussi l'avantage en gÃ©nÃ©ral des outils Hashicorp la configuration est trÃ¨s descriptive et donc comprÃ©hensible rapidement.
On va renseigner les informations suivantes : 
- Des credentials pour le proxmox
- Un peu de hardware pour le template (RAM, CPU, Disk etc...)
- Une config pour le template (ID, nom, boot command , ssh cred etc...)
- Un dossier pour la conf subiquity
- **On active cloud-init car sinon on ne peut pas set les IP via Terraform**
- On tweak un peu l'image car on veut qu'elle soit cloud-init ready

Justement concernant autoinstall depuis la version 20.04 preseed a Ã©tÃ© dÃ©laissÃ© au profit de subiquity qui est (de mon point de vue) bien plus facile Ã  utiliser car format yaml et s'intÃ¨gre trÃ¨s bien avec Packer. Ce qui donne deux fichiers : 
- `meta-data` : requis. UtilisÃ© par le cloud vu qu'on dÃ©ploit en local on le laisse vide
- `user-data` : l'Ã©quivalent du preseed, utilise autoinstall

Le fichier `user-data` en dÃ©tail :
```yml
#cloud-config
autoinstall:
  version: 1
  locale: en_US
  keyboard:
    layout: fr
  ssh:
    install-server: true
    allow-pw: true
  packages:
    - qemu-guest-agent
  user-data:
    users:
        - name: canadium
          passwd: $6$exDY1mhS4KUYCE/2$zmn9ToZwTKLhCw.b4/b.ZRTIZM30JZ4QrOQ2aOXJ8yk96xpcCof0kxKwuX1kqLG/ygbJ1f8wxED22bTL4F46P0
          groups: [adm, cdrom, dip, plugdev, sudo]
          lock-passwd: false
          sudo: ALL=(ALL) NOPASSWD:ALL
          shell: /bin/bash
          ssh_authorized_keys:
            - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJEXrwiuUOCpWPvwOsGuF4K+aq1ufToGMi4ra/1omOZb
```

Fichier de configuration ultra basique, on dÃ©finit un seul user et deux conf pour le systÃ¨me. Les autres options peuvent Ãªtre trouvÃ©es dans la [doc](https://ubuntu.com/server/docs/install/autoinstall-reference)  
âš ï¸ Ce fichier est "templatiser" et redÃ©fini via Terraform voir plus bas

On peut suite Ã©xÃ©cutera ensuite packer via Terraform pour avoir une seule et mÃªme exÃ©cution  
Ce qui nous donne un template de qualitÃ© ! 
![](./template-proxmox.png)

# Terraform

Il faut maintenant dÃ©ployer notre templates sous forme de VM, on va utiliser terraform pour cela qui va Ã©galement nous prÃ©parer nos fichiers ansible.
On installe [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started) puis on s'attaque aux fichier de configuration : 
```
â”œâ”€â”€ main.tf
â”œâ”€â”€ output.tf
â”œâ”€â”€ provider.tf
â”œâ”€â”€ terraform.tfvars
â””â”€â”€ variables.tf
```

Listons les fichiers terraform :
- `main.tf` : On dÃ©crit notre dÃ©ploiement, le fichier de job.
- `output.tf` : La sortie voulue lors de l'exÃ©cution de terraform apply (on va print les IP ici)
- `provider.tf` : On prÃ©cise quel provider on utilise, ici proxmox. (voir [doc](https://registry.terraform.io/providers/Telmate/proxmox/latest/docs))
- `terraform.tfvars` : la dÃ©finition des variables pour le main.tf (mÃªme principe que Packer)
- `variables.tf` : la dÃ©finition des variables par dÃ©faut (mÃªme principe que Packer)

Et on a des fichiers de template qui va nous permettre de dÃ©finir notre inventory ansible, groups_vars ansible, user-data de autoinstall etc...

Je vais juste dÃ©crire le fichier main.tf car les autres sont plutÃ´t Ã©vident. 
On dÃ©finit les variables pour l'exÃ©cution future de packer : 

```ruby
locals {
  template_folder = "${path.module}/${var.folder_packer}/${var.distrib_folder}"
  packer_cfg = {
    PKR_VAR_proxmox_hostname  = var.pm_host
    PKR_VAR_proxmox_username  = var.pm_user
    PKR_VAR_proxmox_password  = var.pm_password
    PKR_VAR_proxmox_node_name = var.pm_node_name
    PKR_VAR_proxmox_insecure_skip_tls_verify = var.pm_tls_insecure

    PKR_VAR_vm_id                 = var.vm_id
    PKR_VAR_vm_name               = var.vm_name
    PKR_VAR_vm_storage_pool       = var.vm_storage_pool
    PKR_VAR_vm_cores              = var.vm_cores
    PKR_VAR_vm_memory             = var.vm_memory
    PKR_VAR_vm_sockets            = var.vm_sockets

    PKR_VAR_iso_url             = var.iso_url
    PKR_VAR_iso_storage_pool    = var.iso_storage_pool
    PKR_VAR_iso_checksum        = var.iso_checksum

    PKR_VAR_http_directory      = var.http_directory

    PKR_VAR_username              = var.username
    PKR_VAR_user_password         = var.user_password
  }
}
```
Ici justement on dÃ©finit le job packer avec l'ensemble des paramÃ¨tres souhaitÃ©s (au final c'est une exÃ©cution shell classique).  
J'ai ajoutÃ© un petit sleep car parfois le template n'Ã©tait pas prÃªt pour l'exÃ©cution Terraform. Egalement un script python pour supprimÃ© le template crÃ©Ã© par Packer (pas de ressource native donc obligÃ© de "hack") : 
```ruby
resource "null_resource" "packer_build" {
    provisioner "local-exec" {
    working_dir = local.template_folder
    command     = "packer build . && sleep 30"
    environment = local.packer_cfg
  }
  provisioner "local-exec" {
    when  = destroy
    command = "${path.module}/scripts/delete_template.py"
    interpreter = ["python"]
    working_dir = path.module
    environment = merge(yamldecode(self.triggers.packer_cfg))
  }
    triggers = {
    packer_cfg = yamlencode(local.packer_cfg)
  }
  depends_on = [
    local_file.user-data
  ]
}
```

On redÃ©finit un master K3S par dessus notre template (var.tamplate_vm_name), on remarque qu'on doit prÃ©ciser une dÃ©pendance pour que les tÃ¢ches d'exÃ©cutent dans le bon ordre. A noter aussi un script sh qui verifiera que le dÃ©ploiement cloud-init est bien fini sur le serveur :
```ruby
resource "proxmox_vm_qemu" "proxmox_vm_master" {
  count       = var.num_k3s_masters
  name        = "k3s-master-${count.index}"
  target_node = var.pm_node_name
  clone       = var.template_vm_name
  os_type     = "cloud-init"
  agent       = 1
  memory      = var.num_k3s_masters_mem
  cores       = var.num_k3s_masters_cpu

  ipconfig0 = "ip=${var.master_ips[count.index]}/${var.networkrange},gw=${var.gateway}"

  lifecycle {
    ignore_changes = [
      ciuser,
      sshkeys,
      disk,
      desc,
      network
    ]
  }

  connection {
    type = "ssh"
    user = var.username
    password = var.user_password
    host = var.worker_ips[count.index]
  }

  provisioner "file" {
    source      = "${path.module}/scripts/wait-cloud-init.sh"
    destination = "/tmp/wait-cloud-init.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/wait-cloud-init.sh",
      "/tmp/wait-cloud-init.sh",
    ]
  }
  depends_on = [
    null_resource.packer_build
  ]
}
```

On fera la mÃªme tÃ¢che pour les workers

Et ici on dÃ©finit nos tÃ¢ches de customisation de conf, c'est Ã  dire :
- GÃ©nÃ©rer des fichiers de configuration pour la future exÃ©cution d'ansible
- GÃ©nÃ©rer le user-data necessaire Ã  l'autoinstall de packer (on modifie essentiellement le user/password)

```ruby
data "template_file" "cloud-init-user-data" {
    template = "${file("${path.module}/templates/user-data.tpl")}"
    vars = {
        SUDO_PASSWORD_HASH = "${var.user_password_hash}"
        SUDO_USERNAME = "${var.username}"
        SSH_PUBLIC_KEY = "${var.ssh_pub_key}"
    }
}
resource "local_file" "user-data" {
    content     = "${data.template_file.cloud-init-user-data.rendered}"
    filename = "${local.template_folder}/http/user-data"
}

data "template_file" "k8s" {
  template = file("./templates/k8s.tpl")
  vars = {
    k3s_master_ip = "${join("\n", [for instance in proxmox_vm_qemu.proxmox_vm_master : join("", [instance.default_ipv4_address, " ansible_ssh_private_key_file=", var.pvt_key])])}"
    k3s_node_ip   = "${join("\n", [for instance in proxmox_vm_qemu.proxmox_vm_workers : join("", [instance.default_ipv4_address, " ansible_ssh_private_key_file=", var.pvt_key])])}"
  }
}

resource "local_file" "k8s_file" {
  content  = data.template_file.k8s.rendered
  filename = "${path.module}/ansible/hosts"
}

data "template_file" "groups_vars_ansible" {
    template = "${file("${path.module}/templates/all.tpl")}"
    vars = {
        ANSIBLE_USER = "${var.username}"
        K3S_VERSION = "${var.k3s_version}"
    }
}

resource "local_file" "groups_vars_ansible" {
    content     = "${data.template_file.groups_vars_ansible.rendered}"
    filename = "${path.module}/ansible/group_vars/all.yml"
}
```
Et enfin le job ansible : 

```ruby
resource "null_resource" "ansible-playbook" {
  provisioner "local-exec" {
    command = "ansible-playbook -i ${var.inventory_file}  --private-key ${var.ssh_key_file} site.yml"
    working_dir = ".."
  }
  depends_on = [
    proxmox_vm_qemu.proxmox_vm_workers,
    proxmox_vm_qemu.proxmox_vm_master,
    local_file.k8s_file,
    local_file.var_file
  ]
}
```

# Ansible

On va enfn dÃ©ployer le cluster K3S via ansible. En voici l'aborescence : 
```
ansible/
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ group_vars
â”‚Â Â  â””â”€â”€ all.yml
â”œâ”€â”€ playbook.yml
â””â”€â”€ roles
    â”œâ”€â”€ download
    â”‚Â Â  â””â”€â”€ tasks
    â”‚Â Â      â””â”€â”€ main.yml
    â”œâ”€â”€ k3s
    â”‚Â Â  â”œâ”€â”€ master
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tasks
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ main.yml
    â”‚Â Â  â”‚Â Â  â””â”€â”€ templates
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ k3s.service.j2
    â”‚Â Â  â”‚Â Â      â””â”€â”€ k3s.service.j2.withoutterafic
    â”‚Â Â  â””â”€â”€ node
    â”‚Â Â      â”œâ”€â”€ tasks
    â”‚Â Â      â”‚Â Â  â””â”€â”€ main.yml
    â”‚Â Â      â””â”€â”€ templates
    â”‚Â Â          â””â”€â”€ k3s.service.j2
    â”œâ”€â”€ postconfig
    â”‚Â Â  â””â”€â”€ localhost
    â”‚Â Â      â””â”€â”€ tasks
    â”‚Â Â          â””â”€â”€ main.yml
    â”œâ”€â”€ prereq
    â”‚Â Â  â”œâ”€â”€ defaults
    â”‚Â Â  â”‚Â Â  â””â”€â”€ main.yml
    â”‚Â Â  â”œâ”€â”€ tasks
    â”‚Â Â  â”‚Â Â  â””â”€â”€ main.yml
    â”‚Â Â  â””â”€â”€ templates
    â”‚Â Â      â””â”€â”€ resolv.conf.j2
    â”œâ”€â”€ raspberrypi
    â”‚Â Â  â”œâ”€â”€ handlers
    â”‚Â Â  â”‚Â Â  â””â”€â”€ main.yml
    â”‚Â Â  â””â”€â”€ tasks
    â”‚Â Â      â”œâ”€â”€ main.yml
    â”‚Â Â      â””â”€â”€ prereq
    â”‚Â Â          â”œâ”€â”€ CentOS.yml
    â”‚Â Â          â”œâ”€â”€ Raspbian.yml
    â”‚Â Â          â”œâ”€â”€ Ubuntu.yml
    â”‚Â Â          â””â”€â”€ default.yml
```

Voici les diffÃ©rents roles : 
- `download` : TÃ©lÃ©charge la release de k3s
- `k3s` : La configuration de k3s pour le master et les nodes (rien de bien compliquÃ©)
- `postconfig` : On configure kubeconfig et installe helm en local sur le server/workstation executant le job ansible
- `prereq` : On installe les prerequis K3S (netfilter, ip forwarding etc...)
- `raspberrypi` : Un job specifique si on est sous Raspberry PI

A noter : 
- Le fichier de group vars all.yml est gÃ©nÃ©rÃ© via Terraform Ã  partir de variables
- Le fichier ansible.cfg peut Ãªtre configurer Ã  votre convenance


# Conclusion 
On a donc crÃ©Ã© un cluster K3s de 0 sans aucune interaction directe ou manuelle avec Proxmox, pas mal non ? En plus de cela tous est dynamique et flexible, il suffit de changer les conf ğŸ˜‰

![](./final_promox.png)


Vous pouvez reprendre ce [projet](https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer) (qui lui mÃªme est forkÃ©). Par rapport Ã  son utilisation tout est expliquÃ© dans le [README](https://github.com/ramuskay/k3s-proxmox-terraform-ansible-packer/blob/main/terraform/README.md)
 et consiste principalement Ã  changer des variables Terraform.

MalgrÃ© tous cela il y a pas mal d'axes d'amÃ©liorations et problÃ¨mes inhÃ©rents Ã  la solution choisie : 
- On pourrait mettre les variables sensibles dans Vault (Outil Hashicorp) et un git qui Ã  chaque push redÃ©ploit une image packer.
- Il faudrait aussi crÃ©Ã©r les utilisateurs adaptÃ©s (terraform & packer) avec les bons droits, voir ressource [Terraform](https://registry.terraform.io/providers/Telmate/proxmox/latest/docs) par exemple.  
- Il faudrait que le job packer se termine au bon moment pour que le job terraform ne se lance pas trop tÃ´t (j'ai un sleep 30 pour l'instant)  
- Pas de provider packer natif donc on doit "tricher" pour supprimer le template
- Terraform considÃ©re Ã  un deuxiÃ¨me run que rien a changÃ© et ne relance pas le job ansible (on peut tout de mÃªme le lancer Ã  la main)

Bref il y a plein d'autres moyens de faire et d'amÃ©liorer ce pipeline, les outils HashiCorp sont quand mÃªme super pour cela !

# Sources

https://www.aerialls.eu/posts/ubuntu-server-2004-image-packer-subiquity-for-proxmox/  
https://tlhakhan.medium.com/ubuntu-server-20-04-autoinstall-2e5f772b655a  
https://stackoverflow.com/questions/72567455/running-cloud-init-twice-via-packer-terraform-on-vmware-ubuntu-22-04-guest  
https://salmonsec.com/blogs/home_lab_3#6-init-kubeadm-images-sh  
https://github.com/blz-ea/proxmox-packer  
https://medium.com/@ssnetanel/build-a-kubernetes-cluster-using-k3s-on-proxmox-via-ansible-and-terraform-c97c7974d4a5